{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Login & install"],"metadata":{"id":"KSeFhM-xtX32"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()\n"],"metadata":{"id":"ie336EA9Wqcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install seqeval\n","!pip install transformers[torch]\n","!pip install accelerate -U\n","!pip install evaluate"],"metadata":{"id":"Ds7eg0CrWWzK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# download dataset & model"],"metadata":{"id":"5z2h-Kiytd1N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3nAjZpwWR_8"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from collections import defaultdict\n","\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import EarlyStoppingCallback\n","from datasets import load_dataset\n","import evaluate"]},{"cell_type":"markdown","source":["## 下載資料集並切分"],"metadata":{"id":"gLjWHJAXtn4E"}},{"cell_type":"code","source":["#%% 讀入 hugging face dataset\n","data = load_dataset(\"jamie613/custom_NER\", data_files = 'train_data_sample.json')\n","#print(data)\n","\n","# 切分為 train & test\n","data = data['train'].train_test_split(test_size = 0.1)\n","train = data['train']\n","test = data['test']\n","print(train)\n","print(test)"],"metadata":{"id":"t_SuNRaQWvwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 計算標籤個數"],"metadata":{"id":"d9BlAEFAPNP7"}},{"cell_type":"code","source":["train_dict = defaultdict(int)\n","test_dict = defaultdict(int)\n","\n","# 只計算 B 的標籤\n","for label in train['labels']:\n","  for l in label:\n","    if l.startswith('B'):\n","      train_dict[l] += 1\n","\n","for label in test['labels']:\n","  for l in label:\n","    if l.startswith('B'):\n","      test_dict[l] += 1\n","\n","df = pd.DataFrame([train_dict, test_dict], index=['train', 'test'])\n","print(df)"],"metadata":{"id":"wJaAnHS4PMqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 製作 label2id & id2label"],"metadata":{"id":"cawuYbEittyk"}},{"cell_type":"code","source":["#%% labels 轉 ner_tags\n","def label_2_id(example):\n","  labels = example['labels']\n","  return {'ner_tags' : [label2id[label] for label in labels]}\n","\n","label2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8,\n","            'B-PERF' : 9, 'I-PERF' : 10, 'B-COMP' : 11, 'I-COMP' : 12, 'B-INST' : 13, 'I-INST' : 14,\n","            'B-OTH' : 15, 'I-OTH' : 16, 'B-MUSIC' : 17, 'I-MUSIC' : 18, 'B-OTHP' : 19, 'I-OTHP' : 20}\n","id2label = {v: k for k, v in label2id.items()}\n","\n","train = train.map(label_2_id)\n","test = test.map(label_2_id)\n","#print(train)\n","#print(train[0])"],"metadata":{"id":"DhJ-x_YM2Qlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 下載預訓練模型 & 分詞器；對齊分詞後的 text 和 ner_tags\n"],"metadata":{"id":"B4NpcS_2t5jT"}},{"cell_type":"code","source":["#%% 對齊分詞後的 text 和 ner_tags\n","# tokenizer 會把 word 拆成 subword\n","# 所以需要對齊\n","model_name = \"bert-base-multilingual-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def align_toeknized_text_and_tags(tokenized_inputs, target_text, original_tags):\n","    align_tags = []\n","    text_idx = 0\n","    token_idx = 0\n","\n","    # 到 [SEP] 前，不然 target_text 會超過\n","    while token_idx < (len(tokenized_inputs) - 1):\n","        token = tokenized_inputs[token_idx]\n","\n","        # 句子頭\n","        if token == '[CLS]':\n","            align_tags.append(-100)\n","            token_idx += 1\n","            continue\n","\n","        # [SEP]、[PAD] 直接全部標 -100\n","        if token == '[SEP]':\n","          align_tags = align_tags[:512] + [-100] *(512 - len(align_tags) - 1)\n","          token_idx = len(tokenized_inputs)\n","          continue\n","\n","        # 這時候再抓 target，不然進到 [SEP] 和 [PAD] 後會報錯\n","        target = target_text[text_idx]\n","\n","        # 比對 token 與 text\n","        #print('current token_idx: ', token_idx)\n","        #print('token: ', token)\n","        #print('current text_idx: ', text_idx)\n","        #print('target text: ', target)\n","        #print()\n","\n","        # 若 token 與目前的 text 完全相符，則直接將 text 對應的 tag 貼上\n","        # 若 token 為 [UNK] 則直接把 text 對應的 tag 貼上\n","        if token == target or token == '[UNK]':\n","            align_tags.append(original_tags[text_idx])\n","            token_idx += 1\n","\n","        # 有 subword 或是標點符號被分離的狀況\n","        else:\n","            # 本字的標籤 (O or B-)\n","            subword_token = original_tags[text_idx]\n","\n","            # target 中所有的字母都對上後， text_idx 才 +1\n","            while target:\n","                #print('init target', target)\n","                # 只跳 token_idx 所以要再讀一次 token\n","                token = tokenized_inputs[token_idx]\n","                #print('token: ', token)\n","                # 移除 subword 前的 ##\n","                token = token.replace('##', '')\n","                # 移除以比對上的 subword\n","                target = target.replace(token, '', 1)\n","                #print('update target: ', target)\n","\n","                # 標籤\n","                if subword_token == 0:\n","                    align_tags.append(0)\n","\n","                elif subword_token % 2 != 0:\n","                    align_tags.append(subword_token)\n","                    subword_token += 1\n","                else:\n","                    align_tags.append(subword_token)\n","\n","                token_idx += 1\n","        text_idx += 1\n","\n","    # 補上 [SEP]\n","    align_tags.append(-100)\n","\n","    return align_tags\n","\n","def tokenize_text(examples):\n","    tokenized_inputs = tokenizer(examples['text'], padding = 'max_length') # 會變成 input_ids, token_type_ids, attention_mask\n","    #print(tokenized_inputs)\n","    #print(tokenized_inputs.tokens())\n","\n","    target_text = examples['text'].split() # 1. 不會拆成兩個，所以對照時會對不上\n","    original_tags = examples['ner_tags']\n","\n","    if len(target_text) > 512:\n","        target_text = target_text[:513]\n","        original_tags = original_tags[:513]\n","\n","    align_tags = align_toeknized_text_and_tags(tokenized_inputs.tokens(), target_text, original_tags)\n","\n","    tokenized_inputs['labels'] = align_tags\n","    return tokenized_inputs\n","\n","tokenized_train = train.map(tokenize_text) # 此時 text 和 ner_tags 不等長\n","tokenized_train = tokenized_train.remove_columns(['ner_tags', 'text'])\n","tokenized_test = test.map(tokenize_text)\n","#print(tokenized_train)\n","#print(tokenized_test)"],"metadata":{"id":"8323bpB1o0aO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 模型訓練"],"metadata":{"id":"L8BCR4u7uE7I"}},{"cell_type":"markdown","source":["## 評估指標"],"metadata":{"id":"2ZwzSJ4Ht_8M"}},{"cell_type":"code","source":["#%% metric 設定\n","metric = evaluate.load('seqeval')\n","\n","def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis = -1)\n","\n","    # 移除 special tokens\n","    predictions = [[p for (p, l) in zip(prediction, label) if l != -100]\n","                   for prediction, label in zip(predictions, labels)]\n","    labels = [[l for l in  label if l != -100] for label in labels]\n","    labels = [[id2label[l] for l in label] for label in labels]\n","    predictions = [[id2label[p] for p in prediction] for prediction in predictions]\n","\n","    all_metrics = metric.compute(references= labels, predictions= predictions,\n","                                 mode = 'strict', scheme = 'IOB2', zero_division = 0)\n","    #print(all_metrics)\n","\n","    return {\n","        'PERF_p' : all_metrics['PERF']['precision'],  # 判斷為 PERF 的，有多少真的是 PERF\n","        'PERF_r' : all_metrics['PERF']['recall'], # 全部的 PERF，有多少正確被判斷\n","        'INST_p' : all_metrics['INST']['precision'],\n","        'INST_r' : all_metrics['INST']['recall'],\n","        'COMP_p' : all_metrics['COMP']['precision'],\n","        'COMP_r' : all_metrics['COMP']['recall'],\n","        'precision' : all_metrics['overall_precision'],\n","        'recall' : all_metrics['overall_recall'],\n","        'f1' : all_metrics['overall_f1'],\n","        'accuracy' : all_metrics['overall_accuracy']\n","        }\n"],"metadata":{"id":"P9hvlEjBErMG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 訓練設定"],"metadata":{"id":"NvUdrysvuIBh"}},{"cell_type":"code","source":["#%% training\n","commit_message = 'dataset=155, epochs=50, batch_size=1, early_stopping=eval_f1'\n","output_name = 'custom_BERT_NER'\n","num_epochs = 50\n","batch_size = 1\n","logging_steps = len(tokenized_train) // batch_size\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name,\n","    num_labels = len(id2label),\n","    id2label = id2label,\n","    label2id = label2id,\n","    ignore_mismatched_sizes=True  # set to True to use custom labels\n","    )\n","\n","args = TrainingArguments(\n","    output_dir = output_name,\n","    per_device_train_batch_size = batch_size,\n","    per_device_eval_batch_size = batch_size,\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    save_total_limit = 2,\n","    learning_rate = 2e-5,\n","    num_train_epochs = num_epochs,\n","    weight_decay = 0.01,\n","    logging_steps = logging_steps,\n","    metric_for_best_model = 'eval_f1',\n","    greater_is_better = True,\n","    load_best_model_at_end = True,\n","    push_to_hub = True\n","    #push_to_hub = False\n","    )\n","\n","trainer = Trainer(\n","    model = model,\n","    args = args,\n","    train_dataset = tokenized_train,\n","    eval_dataset = tokenized_test,\n","    data_collator = data_collator,\n","    compute_metrics = compute_metrics,\n","    tokenizer = tokenizer,\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n","    )\n","\n","trainer.train()\n","trainer.push_to_hub(commit_message = commit_message)\n"],"metadata":{"id":"jM1KhsvnW5vU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 預測"],"metadata":{"id":"d3mLS2ZnuVRp"}},{"cell_type":"code","source":["# 預測\n","from transformers import pipeline\n","import torch\n","import pandas as pd\n","import re\n","import unicodedata\n","\n","def cleaning_text(text):\n","    return re.sub(r'[\\u3000\\xa0\\n\\t]', ' ', text)\n","\n","full_width = str.maketrans('''\n","                           ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ\n","                           ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ\n","                           １２３４５６７８９０\n","                           ''',\n","                           '''\n","                           ABCEDFGHIJKLMNOPQRSTUVWXYZ\n","                           abcdefghijklmnopqrstuvwxyz\n","                           1234567890\n","                           ''')"],"metadata":{"id":"PAAkHOH6qlWd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 預測文字"],"metadata":{"id":"OjZZXOpjuY3Q"}},{"cell_type":"code","source":["text_1 ='''\n","\"nicht Bach, sondern Meer.\"\n","「他的名字不應該是小溪，應該是大海。」\n","貝多芬曾如此感嘆。巴赫的作品，精巧複雜，為歷代音樂人必須下苦功探索的功課。鋼琴博士林聖縈每隔幾年就會演出郭德堡變奏曲，作為對人生階段的回顧。上回演出為2018年，之後世界經歷翻天覆地的變化，因此2023年獨奏會，將再度演出郭德堡變奏曲全曲，檢視五年來的轉變，搭配第五號法國組曲與D小調夏康舞曲，自精巧架構的作品中，流瀉繽紛多彩的樂音。\n","'''\n","\n","text_2 = '''\n","位在伊比利半島上的西班牙，擁有獨特的文化傳承，吸引著世界各地的人們。然而在長途旅行不甚便利的19、20世紀交際，作曲家僅能依靠小說、遊記、書信，甚至其他人的藝術作品，來認識西班牙風情，進而譜寫以西班牙為主題的「伊比利風味」作品。男低音羅俊穎挑選法、俄作曲家隔空描繪西班牙風景、舞蹈和人們的作品，對照西班牙作曲家法雅以西班牙傳統歌謠寫成、原汁原味的《七首西班牙民歌》，想像與真實各有獨特魅力！\n","男低音 / 羅俊穎\n","鋼　琴 / 翁重華\n","2024年1月19日（五）晚 於臺中國家歌劇小劇場 500\n","2023年12月21日（四）晚 於國家兩廳院演奏廳 400 / 600 / 800\n","購票請洽：https://bit.ly/Lo_Spain 02-3393-9888及FamilyMart全家便利商店、7-ELEVEN超商。\n","兩廳院會員9折；臺中國家歌劇院會員9折、忘我會員75折；學生8折；身障及必要陪同者一名、65歲以上長者5折。\n","'''\n","\n","text_3 = '''\n","演出曲目：\n","Georges Bizet 比才\n","　1. Le Matin 早晨\n","　2. Pastorale 田園\n","　3. Serenade 夜曲\n","Jules Massenet 馬斯奈\n","\"Nuit d'Espagne\" 西班牙之夜\n","Maurice Ravel 拉威爾\n","Pièce en forme de habanera 哈巴奈拉風的練習曲\n","Maurice Ravel 拉威爾\n","\"Don Quichotte à Dulcinée\" 唐吉軻德致杜辛妮\n","　1. Chanson romanesque 浪漫曲\n","　2. Chanson épique 敘事詩\n","　3. Chanson à boire 飲酒歌\n","Mikhail Ivanovich Glinka 葛令卡\n","　1. Ночной зефир 夜的微風\n","　2. Я здесь, Инезилья 我在這兒！伊內齊拉\n","　3. Болеро 波麗路舞曲\n","Pyotr Ilyich Tchaikovsky 柴可夫斯基\n","\"Серенада Дон Жуана\" 唐璜小夜曲\n","Dmitri Shostakovich 蕭士塔高維契\n","\"Испанские песни, op. 100\" 西班牙歌曲，作品100\n","　1. Прощай, Гренада ! 再會！格拉納達\n","　2. Звёздочки 小星星\n","　3. Первая встреча 初相見\n","　4. Ронда 隆達\n","　5. Черноокая 黑眼睛的姑娘\n","　6. Сон ( Баркарола) 夢（船歌）\n","Manuel de Falla 法雅\n","\"Siete canciones populares Españolas\" 七首西班牙民歌\n","　1. El paño moruno 摩爾人的布\n","　2. Seguidilla murciana 莫夕亞地區的賽圭第亞舞曲\n","　3. Asturiana 阿斯圖里亞那民謠\n","　4. Jota 荷他舞曲\n","　5. Nana 搖籃曲\n","　6. Canción 歌謠\n","　7. Polo 舞曲\n","\n","主辦單位：Legato樂聚\n","贊助單位：國藝會\n","'''\n","\n","text_4 = '''\n","女高音林小美受教於王大明、陳白白，林小美曾與吉他孫欣欣一起灌錄威爾第的作品「四季」，專輯獲金曲獎肯定。\n","也曾在伯恩斯坦的指揮下，演出威爾第鋼琴協奏曲。\n","本次將演出比才歌劇《卡門》中的〈飲酒歌〉。卡門強烈的性格，讓這齣歌劇頗受觀眾喜愛。\n","'''"],"metadata":{"id":"RACZuvaeqqCg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 預測結果"],"metadata":{"id":"3_eO7Vi9ugPk"}},{"cell_type":"code","source":["#text = text_1 # 郭德堡\n","#text = text_2 # 伊比利\n","#text = text_3 # 伊比利曲目\n","text = text_4\n","\n","# 文字整理\n","text = cleaning_text(text)\n","# 把全形數字、英文字母轉為半形\n","text = text.translate(full_width)\n","\n","# 訓練後直接預測\n","# 沒有重新讀入模型和分詞器\n","inputs = tokenizer(text, return_tensors = 'pt')\n","inputs = inputs.to('cuda') # 如果不是從 huggingface 下載模型\n","\n","# 如果要從 huggingface 下載模型\n","#model_name = 'jamie613/custom_BERT_NER'\n","#tokenizer = AutoTokenizer.from_pretrained(model_name)\n","#nlp = pipeline('ner', model = model_name, tokenizer = tokenizer, grouped_entities = True, device = 0)\n","\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","predictions = torch.argmax(logits, axis = -1).tolist() # 包含頭尾\n","#print([[id2label[p] for p in predict] for predict in predictions])\n","\n","tokens = inputs['input_ids'].tolist()[0]\n","for i in range(0, len(tokens), 10):\n","  substring = []\n","  for j in range(10):\n","    substring.append(tokenizer.decode(tokens[min((i + j), len(tokens) - 1)]))\n","  sub_pred = [id2label[p] for p in predictions[0][i : i + 10]]\n","  df = pd.DataFrame([substring, sub_pred])\n","  print(df.to_markdown())\n","  print()\n"],"metadata":{"id":"0zxkhFS9aOIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 用 pipeline 預測\n","nlp = pipeline('ner', model = model, tokenizer = tokenizer, grouped_entities = True, device = 0)\n","#print(nlp(text))\n","\n","res = nlp(text)\n","#print(res)\n","\n","ner = tokenizer.decode(tokenizer(text)['input_ids']).split()\n","# 去除頭尾的 [CLS] 和 [SEP]\n","# 原本句子中的空格會消失\n","ner = ner[1 : -1]\n","ner_tag = ['O'] * len(ner)\n","#print(ner_tag)\n","\n","# 比對目前 entity_group 中的 word 是 tokenizer.decode()\n","text_idx = 0\n","ner_idx = 0\n","\n","ner_length = len(ner)\n","\n","for i in range(len(res)):\n","    left = res[i]['start']\n","    right = res[i]['end']\n","\n","    while text_idx < right:\n","        #print('current ner:', ner[ner_idx], ner_idx)\n","        #print('current text: ', text[text_idx : text_idx + 3], text_idx)\n","        while text[text_idx] == ' ':\n","            text_idx += 1\n","        if text_idx == left:\n","            ner_tag[ner_idx] = 'B-' + res[i]['entity_group']\n","            #print(text[text_idx], 'tag: ', ner_tag[ner_idx])\n","        if text_idx > left and text_idx < right:\n","            ner_tag[ner_idx] = 'I-' + res[i]['entity_group']\n","            #print(text[text_idx], 'tag: ', ner_tag[ner_idx])\n","        text_idx += len(ner[ner_idx])\n","        ner_idx += 1\n","        #print()\n","\n","df = pd.DataFrame([ner, ner_tag])\n","\n","for i in range(0, len(ner_tag), 10):\n","    print(df.iloc[:, np.r_[i : min(i+10, len(ner_tag))]].to_markdown())\n","    print()"],"metadata":{"id":"gXTcKFB9rDtq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Confusion Matrix"],"metadata":{"id":"qy-tP2LQJxEP"}},{"cell_type":"code","source":["from transformers import pipeline\n","import torch\n","import pandas as pd\n","\n","# 從 huggingface 下載模型\n","cm_model_name = 'jamie613/custom_BERT_NER'\n","tokenizer = AutoTokenizer.from_pretrained(cm_model_name)\n","model = AutoModelForTokenClassification.from_pretrained(cm_model_name)\n","nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities = True)\n","\n","label2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8,\n","            'B-PERF' : 9, 'I-PERF' : 10, 'B-COMP' : 11, 'I-COMP' : 12, 'B-INST' : 13, 'I-INST' : 14,\n","            'B-OTH' : 15, 'I-OTH' : 16, 'B-MUSIC' : 17, 'I-MUSIC' : 18, 'B-OTHP' : 19, 'I-OTHP' : 20}\n","id2label = {v: k for k, v in label2id.items()}"],"metadata":{"id":"oZ_t5VZgJ-xB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_predictions(text):\n","  inputs = tokenizer(text, return_tensors = 'pt')\n","  with torch.no_grad():\n","      logits = model(**inputs).logits\n","  predictions = torch.argmax(logits, axis = -1).tolist() # 包含頭尾\n","  pred_tags = [id2label[p] for p in predictions[0]][1 : -1]\n","  # 將分詞後的 input_ids 取出，將 tesnor 轉為 list，刪除 [CLS]、[SEP]\n","  inputs = inputs['input_ids'].tolist()[0][1:-1]\n","  # input_ids 轉回 tokens；subword 前有 ##\n","  inputs = tokenizer.convert_ids_to_tokens(inputs)\n","\n","  # 還原文句、對齊標籤\n","  text = []\n","  ner = []\n","  for i in range(len(pred_tags)):\n","      if not inputs[i].startswith('##'):\n","          text.append(inputs[i])\n","          ner.append(pred_tags[i])\n","      # subword 刪掉前面的 ## 後，併到前面的字\n","      else:\n","          text[-1] += inputs[i].replace('##', '')\n","\n","  return ner"],"metadata":{"id":"uCbz70pXODia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actual_labels = []\n","pred_labels = []\n","for i in range(len(test)):\n","  pred_labels.extend(test_predictions(test[i]['text']))\n","  actual_labels.extend(test[i]['labels'])\n"],"metadata":{"id":"R5AI05BpSjSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","\n","cm = metrics.confusion_matrix(actual_labels, pred_labels,\n","                              labels = ['B-PERF', 'I-PERF',\n","                                        'B-COMP', 'I-COMP',\n","                                        'B-INST', 'I-INST',\n","                                        'B-MUSIC', 'I-MUSIC',\n","                                        'B-PER', 'I-PER',\n","                                        'B-OTH', 'I-OTH',\n","                                        'B-OTHP', 'I-OTHP',\n","                                        'B-ORG', 'I-ORG',\n","                                        'B-LOC', 'I-LOC',\n","                                        'B-MISC', 'I-MISC',\n","                                        'O'])\n","\n","cm = pd.DataFrame(cm, index = ['B-PERF', 'I-PERF',\n","                               'B-COMP', 'I-COMP',\n","                               'B-INST', 'I-INST',\n","                               'B-MUSIC', 'I-MUSIC',\n","                               'B-PER', 'I-PER',\n","                               'B-OTH', 'I-OTH',\n","                               'B-OTHP', 'I-OTHP',\n","                               'B-ORG', 'I-ORG',\n","                               'B-LOC', 'I-LOC',\n","                               'B-MISC', 'I-MISC',\n","                               'O'],\n","                  columns = ['B-PERF', 'I-PERF',\n","                             'B-COMP', 'I-COMP',\n","                             'B-INST', 'I-INST',\n","                             'B-MUSIC', 'I-MUSIC',\n","                             'B-PER', 'I-PER',\n","                             'B-OTH', 'I-OTH',\n","                             'B-OTHP', 'I-OTHP',\n","                             'B-ORG', 'I-ORG',\n","                             'B-LOC', 'I-LOC',\n","                             'B-MISC', 'I-MISC',\n","                             'O'])"],"metadata":{"id":"d4MrDzxCOqbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 刪除 'O' 的行、列\n","cm_sub = cm.iloc[:-1, :-1]\n","\n","#製作遮罩\n","mask_df = pd.DataFrame(np.diag(np.full(cm_sub.shape[0], True)))\n","\n","for i in range(len(mask_df)):\n","  for j in range(len(mask_df)):\n","    if cm_sub.iloc[i][j] == 0:\n","      mask_df.iloc[i][j] = True\n","\n","ax = sns.heatmap(cm_sub, cmap = 'Blues', vmin = 0,\n","                 linewidth = 0.5, linecolor = 'gray',\n","                 annot = True, mask = mask_df.to_numpy())\n","ax.plot(ax.get_xlim(), ax.get_xlim(), linestyle = '-.')\n","plt.show()"],"metadata":{"id":"hCo2N_zuVRf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = test.to_pandas()\n","test_df.to_csv('test_df.csv', encoding = 'utf-8-sig', index = False)"],"metadata":{"id":"Iiu-_kkn4Q8g"},"execution_count":null,"outputs":[]}]}